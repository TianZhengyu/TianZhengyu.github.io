<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>A Multimodal BiMamba Network with Test-Time Adaptation for Emotion Recognition Based on Physiological Signals</title>

  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;600;800&display=swap" rel="stylesheet">

  <style>
    :root{
      --max-w: 1200px;
      --muted: #6b7280;
      --accent: #1f2937;
      --bg: #fff;
    }
    html,body{
      height:100%;
      margin:0;
      background:var(--bg);
      font-family:"Montserrat",system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial;
      color:#111827;
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
    }
    .container{
      max-width:var(--max-w);
      margin:0 auto;
      padding:48px 20px;
      display:flex;
      flex-direction:column;
      align-items:center;
    }
    header{
      text-align:center;
      width:100%;
    }
    h1{
      margin:0;
      font-weight:800;
      line-height:1.04;
      letter-spacing:-0.02em;
      font-size:clamp(28px,6.5vw,56px);
      margin-bottom:18px;
    }
    .authors{
      color:var(--muted);
      font-size:15px;
      line-height:1.5;
      margin-bottom:8px;
    }
    .affil{
      color:var(--muted);
      font-size:14px;
      margin-top:8px;
    }
    .meta{
      margin-top:6px;
      color:var(--muted);
      font-size:14px;
    }
    .btns{
      margin-top:18px;
      display:flex;
      gap:12px;
      justify-content:center;
      align-items:center;
      flex-wrap:wrap;
    }
    .btn{
      display:inline-flex;
      align-items:center;
      gap:8px;
      padding:10px 16px;
      border-radius:999px;
      text-decoration:none;
      font-weight:600;
      font-size:15px;
      box-shadow:0 6px 18px rgba(15,23,42,0.06);
    }
    .btn-paper{
      background:var(--accent);
      color:#fff;
    }
    .btn-code{
      background:transparent;
      color:var(--accent);
      border:1px solid rgba(17,24,39,0.12);
    }

    main{
      width:100%;
      margin-top:36px;
      display:flex;
      flex-direction:column;
      align-items:center;
      gap:56px;
    }

    .figure-box{
      width:100%;
      border-radius:18px;
      border:3px dashed rgba(107,114,128,0.25);
      padding:22px;
      box-sizing:border-box;
      display:flex;
      justify-content:center;
      align-items:center;
      background:linear-gradient(180deg,rgba(255,255,255,0.6),rgba(255,255,255,0.4));
    }
    .figure-box img{
      max-width:100%;
      height:auto;
      border-radius:8px;
      display:block;
      box-shadow:inset 0 -6px 18px rgba(0,0,0,0.03);
    }

    /* 统一 PDF iframe 样式 + 不同高度 */
    .pdf-frame{
      width:100%;
      border:0;
      border-radius:8px;
      box-shadow:inset 0 -6px 18px rgba(0,0,0,0.03);
    }
    /* 顶部 paper figure：略小于整屏高度，减少底部留白，同时 FitV 显示整张图 */
    .pdf-frame-wide{
      height:80vh;
    }
    .pdf-frame-tall{
      height:680px;
    }

    /* 只缩小 experiment-1.png */
    .exp1-img{
      width:66%;
      height:auto;
    }

    /* 顶部关键段落高亮卡片 */
    .figure-caption-top{
      max-width:900px;
      margin:16px auto 0;
      padding:14px 18px;
      font-size:17px;
      line-height:1.8;
      color:#111827;
      background:#fefce8;
      border-radius:14px;
      border:1px solid #facc15;
      box-shadow:0 10px 30px rgba(15,23,42,0.08);
    }
    .figure-caption-top strong{
      font-weight:700;
    }

    .section{
      width:100%;
    }
    .section-title{
      text-align:center;
      font-size:26px;
      font-weight:700;
      margin:0 0 18px;
    }

    /* 让小标题更突出：颜色更深、稍大、带浅色背景条 */
    .section-subtitle{
      font-size:19px;
      font-weight:700;
      margin:0 0 8px;
      color:var(--accent);
      display:inline-block;
      padding:4px 12px;
      border-radius:999px;
      background:rgba(15,23,42,0.06);
    }

    /* Motivation / Our Approach 标签样式 */
    .inline-tag{
      display:inline-block;
      padding:2px 10px;
      margin-right:6px;
      border-radius:999px;
      background:#e5e7eb;
      color:#111827;
      font-weight:700;
      font-size:14px;
      letter-spacing:0.02em;
    }

    .section-content{
      max-width:900px;
      margin:0 auto;
      font-size:16px;
      line-height:1.8;
      color:var(--muted);
    }
    .section--soft{
      padding:32px 20px;
      border-radius:18px;
      background:#f9fafb;
    }

    .method-block{
      max-width:1000px;
      margin:0 auto;
    }

    .figure-box.small{
      padding:16px;
    }

    .fig-caption{
      text-align:center;
      font-size:15px;
      color:var(--muted);
      margin-top:8px;
    }

    .video-center{
      max-width:900px;
      margin:0 auto;
      text-align:center;
    }
    .video-tabs{
      display:inline-flex;
      border-radius:999px;
      border:1px solid #e5e7eb;
      overflow:hidden;
      margin-bottom:18px;
    }
    .video-tab{
      padding:8px 18px;
      font-size:15px;
      cursor:default;
      color:var(--muted);
      white-space:nowrap;
    }
    .video-tab.is-active{
      background:#111827;
      color:#fff;
    }
    .video-box{
      width:100%;
      margin:0 auto 12px;
    }
    .video-box video{
      width:100%;
      border-radius:14px;
      box-shadow:0 12px 32px rgba(15,23,42,0.25);
      background:#000;
    }
    .btn-row{
      margin-top:10px;
      display:flex;
      flex-wrap:wrap;
      justify-content:center;
      gap:12px;
    }

    .poster-frame{
      width:100%;
      max-width:1000px;
      margin:0 auto;
      border-radius:14px;
      overflow:hidden;
      border:1px solid #e5e7eb;
      background:#f9fafb;
    }

    .bibtex-block{
      max-width:1000px;
      margin:0 auto;
      background:#111827;
      color:#e5e7eb;
      padding:18px 20px;
      border-radius:14px;
      font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New";
      font-size:14px;
      overflow-x:auto;
      box-sizing:border-box;
    }

    /* 稍微弱化正文粗体，让它不要和小标题抢视觉中心 */
    strong{
      font-weight:600;
    }

    footer{
      margin-top:32px;
      color:var(--muted);
      font-size:14px;
      text-align:center;
    }

    @media (max-width:640px){
      .container{
        padding:28px 14px;
      }
      h1{
        font-size:clamp(22px,8.5vw,34px);
      }
      .authors{
        font-size:14px;
      }
      .btn{
        padding:8px 12px;
        font-size:14px;
      }
      main{
        gap:40px;
      }
      /* 小屏上按视口高度展示整张 figure */
      .pdf-frame-wide{
        height:90vh;
      }
      .pdf-frame-tall{
        height:520px;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>
        A Multimodal BiMamba Network with Test-Time Adaptation<br>
        for Emotion Recognition Based on Physiological Signals
      </h1>

      <div class="authors">
        Ziyu Jia<sup>1,2</sup>, Tingyu Du<sup>3,4</sup>, Zhengyu Tian<sup>5</sup>, Hongkai Li<sup>5</sup>, Yong Zhang<sup>6*</sup>, Chenyu Liu<sup>7*</sup>
        <div class="affil">1 Institute of Automation, Chinese Academy of Sciences</div>
        <div class="affil">2 Shanghai Key Laboratory of Data Science</div>
        <div class="affil">3 Beijing Key Laboratory of Mobile Computing and Pervasive Devices, Institute of Computing Technology, Chinese Academy of Sciences</div>
        <div class="affil">4 University of Chinese Academy of Sciences</div>
        <div class="affil">5 School of Computer Science and Technology, Beijing Jiaotong University</div>
        <div class="affil">6 School of Information Engineering, Huzhou University</div>
        <div class="affil">7 College of Computing and Data Science, Nanyang Technological University</div>
      </div>
      <div class="meta">
        <div>NeurIPS 2025</div>
        <div style="color:var(--muted);font-size:14px">*Corresponding Author</div>
      </div>

      <div class="btns">
        <a class="btn btn-paper" href="extension://ngbkcglbmlglgldjfcnhaijeecaccgfi/https://openreview.net/pdf?id=3vLp3J7540">Paper</a>
        <a class="btn btn-code" href="https://neurips.cc/public/EthicsGuidelines">Code</a>
      </div>
    </header>

    <main>
      <!-- 顶部方法大图：嵌入同目录下的 module-all.pdf -->
      <section class="figure-box">
        <iframe
          class="pdf-frame pdf-frame-wide"
          src="module-all.pdf#view=FitV">
        </iframe>
      </section>

      <!-- 顶部 figure 描述高亮卡片 -->
      <div class="figure-caption-top">
        <strong>
          In this paper, we propose a multimodal BiMamba network with TTA for emotion recognition.
          The multimodal BiMamba network effectively captures intra-modal dependencies and inter-modal correlations of multimodal physiological signals.
          TTA consists of two-level entropy-based sample filtering and mutual information sharing across modalities, which together achieve smooth adaptation to the target domain and reduce distribution shifts across modalities, thereby alleviating the negative impact of amplified distribution shifts caused by missing multimodal data.
        </strong>
      </div>

      <!-- 1. Abstract 区：灰 -->
      <section class="section section--soft">
        <h2 class="section-title">Abstract</h2>
        <div class="section-content">
          <p>
             Emotion recognition based on physiological signals plays a vital role in psychological health and human–computer interaction, particularly with the substantial advances in multimodal emotion recognition techniques. However, two key challenges remain unresolved: <strong>1) how to effectively model the intra-modal long-range dependencies and inter-modal correlations in multimodal physiological emotion signals</strong>, and <strong>2) how to address the performance limitations resulting from missing multimodal data</strong>. In this paper, we propose <strong>a multimodal bidirectional Mamba (BiMamba) network with test-time adaptation (TTA)</strong> for emotion recognition named <strong>BiM-TTA</strong>. Specifically, <strong>BiM-TTA</strong> consists of <strong>a multimodal BiMamba network</strong> and <strong>a multimodal TTA</strong>. The former includes <strong>intra-modal and inter-modal BiMamba modules</strong>, which model long-range dependencies along the time dimension and capture cross-modal correlations along the channel dimension, respectively. The latter (TTA) mitigates the amplified distribution shifts caused by missing multimodal data through <strong>two-level entropy-based sample filtering</strong> and <strong>mutual information sharing acro
