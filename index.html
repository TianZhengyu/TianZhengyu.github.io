<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>A Multimodal BiMamba Network with Test-Time Adaptation for Emotion Recognition Based on Physiological Signals</title>

  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;600;800&display=swap" rel="stylesheet">

  <style>
    :root{
      --max-w: 1200px;
      --muted: #6b7280;
      --accent: #1f2937;
      --bg: #fff;
    }
    html,body{
      height:100%; margin:0; background:var(--bg);
      font-family:"Montserrat",system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial;
      color:#111827;
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
    }
    .container{
      max-width:var(--max-w);
      margin:0 auto;
      padding:48px 20px;
      display:flex;
      flex-direction:column;
      align-items:center;
    }
    header{ text-align:center; width:100%; }
    h1{
      margin:0;
      font-weight:800;
      line-height:1.04;
      letter-spacing:-0.02em;
      font-size:clamp(28px,6.5vw,56px);
      margin-bottom:18px;
    }
    .authors{
      color:var(--muted);
      font-size:15px;
      line-height:1.5;
      margin-bottom:8px;
    }
    .affil{
      color:var(--muted);
      font-size:14px;
      margin-top:8px;
    }
    .meta{
      margin-top:6px;
      color:var(--muted);
      font-size:14px;
    }
    .btns{
      margin-top:18px;
      display:flex;
      gap:12px;
      justify-content:center;
      align-items:center;
      flex-wrap:wrap;
    }
    .btn{
      display:inline-flex;
      align-items:center;
      gap:8px;
      padding:10px 16px;
      border-radius:999px;
      text-decoration:none;
      font-weight:600;
      font-size:15px;
      box-shadow:0 6px 18px rgba(15,23,42,0.06);
    }
    .btn-paper{
      background:var(--accent);
      color:#fff;
    }
    .btn-code{
      background:transparent;
      color:var(--accent);
      border:1px solid rgba(17,24,39,0.12);
    }

    main{
      width:100%;
      margin-top:36px;
      display:flex;
      flex-direction:column;
      align-items:center;
      gap:56px;
    }

    .figure-box{
      width:100%;
      border-radius:18px;
      border:3px dashed rgba(107,114,128,0.25);
      padding:22px;
      box-sizing:border-box;
      display:flex;
      justify-content:center;
      align-items:center;
      background:linear-gradient(180deg,rgba(255,255,255,0.6),rgba(255,255,255,0.4));
    }
    .figure-box img{
      max-width:100%;
      height:auto;
      border-radius:8px;
      display:block;
      box-shadow:inset 0 -6px 18px rgba(0,0,0,0.03);
    }

    /* 统一 PDF iframe 样式 + 不同高度 */
    .pdf-frame{
      width:100%;
      border:0;
      border-radius:8px;
      box-shadow:inset 0 -6px 18px rgba(0,0,0,0.03);
    }
    .pdf-frame-wide{
      /* module-all.pdf：横向结构，稍矮一点 */
      height:480px;
    }
    .pdf-frame-tall{
      /* poster.pdf：纵向较长，给更大高度 */
      height:680px;
    }

    /* 只缩小 experiment-1.png */
    .exp1-img{
      width:66%;
      height:auto;
    }

    .figure-caption-top{
      max-width:900px;
      margin:8px auto 0;
      font-size:17px;
      line-height:1.8;
      color:var(--muted);
      text-align:center;
      font-weight:600;
    }

    .section{
      width:100%;
    }
    .section-title{
      text-align:center;
      font-size:26px;
      font-weight:700;
      margin:0 0 18px;
    }
    .section-subtitle{
      font-size:18px;
      font-weight:700;
      margin:0 0 8px;
    }
    .section-content{
      max-width:900px;
      margin:0 auto;
      font-size:16px;
      line-height:1.8;
      color:var(--muted);
    }
    .section--soft{
      padding:32px 20px;
      border-radius:18px;
      background:#f9fafb;
    }

    .method-block{
      max-width:1000px;
      margin:0 auto;
    }

    .figure-box.small{
      padding:16px;
    }

    .fig-caption{
      text-align:center;
      font-size:15px;
      color:var(--muted);
      margin-top:8px;
    }

    .video-center{
      max-width:900px;
      margin:0 auto;
      text-align:center;
    }
    .video-tabs{
      display:inline-flex;
      border-radius:999px;
      border:1px solid #e5e7eb;
      overflow:hidden;
      margin-bottom:18px;
    }
    .video-tab{
      padding:8px 18px;
      font-size:15px;
      cursor:default;
      color:var(--muted);
      white-space:nowrap;
    }
    .video-tab.is-active{
      background:#111827;
      color:#fff;
    }
    .video-box{
      width:100%;
      margin:0 auto 12px;
    }
    .video-box video{
      width:100%;
      border-radius:14px;
      box-shadow:0 12px 32px rgba(15,23,42,0.25);
      background:#000;
    }
    .btn-row{
      margin-top:10px;
      display:flex;
      flex-wrap:wrap;
      justify-content:center;
      gap:12px;
    }

    .poster-frame{
      width:100%;
      max-width:1000px;
      margin:0 auto;
      border-radius:14px;
      overflow:hidden;
      border:1px solid #e5e7eb;
      background:#f9fafb;
    }

    .bibtex-block{
      max-width:1000px;
      margin:0 auto;
      background:#111827;
      color:#e5e7eb;
      padding:18px 20px;
      border-radius:14px;
      font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New";
      font-size:14px;
      overflow-x:auto;
      box-sizing:border-box;
    }

    footer{
      margin-top:32px;
      color:var(--muted);
      font-size:14px;
      text-align:center;
    }

    @media (max-width:640px){
      .container{ padding:28px 14px;}
      h1{ font-size:clamp(22px,8.5vw,34px);}
      .authors{font-size:14px;}
      .btn{padding:8px 12px;font-size:14px;}
      main{ gap:40px;}
      /* 小屏高度再调小一点，避免太长 */
      .pdf-frame-wide{ height:320px;}
      .pdf-frame-tall{ height:520px;}
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>
        A Multimodal BiMamba Network with Test-Time Adaptation<br>
        for Emotion Recognition Based on Physiological Signals
      </h1>

      <div class="authors">
        Ziyu Jia<sup>1,2</sup>, Tingyu Du<sup>3,4</sup>, Zhengyu Tian<sup>5</sup>, Hongkai Li<sup>5</sup>, Yong Zhang<sup>6*</sup>, Chenyu Liu<sup>7*</sup>
        <div class="affil">1 Institute of Automation, Chinese Academy of Sciences</div>
        <div class="affil">2 Shanghai Key Laboratory of Data Science</div>
        <div class="affil">3 Beijing Key Laboratory of Mobile Computing and Pervasive Devices, Institute of Computing Technology, Chinese Academy of Sciences</div>
        <div class="affil">4 University of Chinese Academy of Sciences</div>
        <div class="affil">5 School of Computer Science and Technology, Beijing Jiaotong University</div>
        <div class="affil">6 School of Information Engineering, Huzhou University</div>
        <div class="affil">7 College of Computing and Data Science, Nanyang Technological University</div>
      </div>
      <div class="meta">
        <div>NeurIPS 2025</div>
        <div style="color:var(--muted);font-size:14px">*Corresponding Author</div>
      </div>

      <div class="btns">
        <a class="btn btn-paper" href="extension://ngbkcglbmlglgldjfcnhaijeecaccgfi/https://openreview.net/pdf?id=3vLp3J7540">Paper</a>
        <a class="btn btn-code" href="https://neurips.cc/public/EthicsGuidelines">Code</a>
      </div>
    </header>

    <main>
      <!-- 顶部方法大图：嵌入同目录下的 module-all.pdf -->
      <section class="figure-box">
        <iframe
          class="pdf-frame pdf-frame-wide"
          src="module-all.pdf#view=FitH">
        </iframe>
      </section>

      <!-- 顶部 figure 描述文字（已加粗） -->
      <div class="figure-caption-top">
        In this paper, we propose a multimodal BiMamba network with TTA for emotion recognition. The multimodal BiMamba network effectively captures intra-modal dependencies and inter-modal correlations of multimodal physiological signals. The two-level entropy-based sample filtering and mutual information sharing across modalities achieve smooth adaptation to the target domain and reduce distribution shifts across modalities, thereby alleviating the negative impact of amplified distribution shifts caused by missing multimodal data.
      </div>

      <!-- Abstract 区 -->
      <section class="section section--soft">
        <h2 class="section-title">Abstract</h2>
        <div class="section-content">
          <p>
             Emotion recognition based on physiological signals plays a vital role in psychological health and human–computer interaction, particularly with the substantial advances in multimodal emotion recognition techniques. However, two key challenges remain unresolved: <strong>1) how to effectively model the intra-modal long-range dependencies and inter-modal correlations in multimodal physiological emotion signals</strong>, and <strong>2) how to address the performance limitations resulting from missing multimodal data</strong>. In this paper, we propose <strong>a multimodal bidirectional Mamba (BiMamba) network with test-time adaptation (TTA)</strong> for emotion recognition named <strong>BiM-TTA</strong>. Specifically, <strong>BiM-TTA</strong> consists of <strong>a multimodal BiMamba network</strong> and <strong>a multimodal TTA</strong>. The former includes <strong>intra-modal and inter-modal BiMamba modules</strong>, which model long-range dependencies along the time dimension and capture cross-modal correlations along the channel dimension, respectively. The latter (TTA) mitigates the amplified distribution shifts caused by missing multimodal data through <strong>two-level entropy-based sample filtering</strong> and <strong>mutual information sharing across modalities</strong>. By addressing these challenges, BiM-TTA achieves <strong>state-of-the-art</strong> results on two multimodal emotion datasets.
          </p>
        </div>
      </section>

      <!-- Methodology 区 -->
      <section class="section">
        <h2 class="section-title">Methodology</h2>

        <!-- 方法 1：Multimodal BiMamba Network，图片在上，文字在下 -->
        <div class="method-block">
          <div class="figure-box small">
            <img src="BiMamba-module.png" alt="BiMamba backbone illustration" />
          </div>
          <div class="fig-caption">
            Figure 1: The intra- and inter-modal BiMamba modules capture and fuse the features of different modalities, respectively. "Concat" represents the concatenation of multiple feature vectors u<sub>1</sub>, u<sub>2</sub>, ..., u<sub>M</sub>. T denotes matrix transposition.
          </div>
          <div class="section-content">
            <h3 class="section-subtitle">1. Multimodal BiMamba Network</h3>
            <p>
              <strong>Motivation &amp; Challenge:</strong> As a typical form of time-series data, emotion-related physiological signals exhibit long-range dependencies reflecting the gradual accumulation of emotional changes. Emotions such as anxiety develop progressively and require a duration for their physiological manifestations to accumulate. This accumulation indicates that emotional changes are not just an instantaneous state but a process that evolves. Inter-modal correlations are also evident in the way different modalities respond. For instance, increases in EEG activity during emotional arousal are often accompanied by galvanic skin response conductance peaks and decreases in electrocardiogram heart-rate variability. Traditional backbone networks have limitations in modeling intra-modal long-range dependencies and inter-modal correlations. For intra-modal modeling, CNN-based networks excel at extracting local features but struggle to capture essential long-range temporal information. Although stacking more convolutional layers together with pooling operations to expand the receptive field can theoretically increase context, it often degrades fine-grained local details. Transformer-based networks are able to capture global emotional patterns, but their attention mechanism lacks explicit temporal filtering capability and tends to distribute weight uniformly across all time steps, which makes it difficult to construct practical long-range dependencies based on key nodes of emotion fluctuations. For inter-modal modeling, the token-level attention in Transformer-based networks only computes pairwise relationships between channels and maintains only instantaneous interaction states. However, the complex dependencies inherent in physiological signals cannot be fully captured through pairwise channel-wise interactions, making it difficult to model high-order correlations. This limitation essentially stems from the lack of explicit global state variables in the attention mechanism, which hinders the effective integration of cross-modal information. Therefore, effectively modeling the intra-modal long-range dependencies and inter-modal correlations of multimodal physiological signals remains a significant challenge.
            </p>
            <p>
              <strong>Our Approach:</strong> 这里概括 MBiM 的核心设计，例如基于 BiMamba 的时间序列建模机制、模态内与模态间双向结构，以及如何在统一框架中对多种生理模态进行对齐与融合，从而更充分地利用 EEG、EOG、ECG 等模态中互补的情感相关信息。
            </p>
          </div>
        </div>

        <!-- 方法 2：Test-Time Adaptation，图片在上，文字在下 -->
        <div class="method-block" style="margin-top:32px">
          <div class="figure-box small">
            <img src="MMTTA.png" alt="Test-Time Adaptation illustration" />
          </div>
          <div class="fig-caption">
            Figure 2: Samples with weak distribution shifts and rich multimodal information are selected for adaptation, while the remaining samples are retained until the next iteration. Then, mutual information sharing across modalities is performed to match the information between different modalities effectively.
          </div>
          <div class="section-content">
            <h3 class="section-subtitle">2. Test-Time Adaptation</h3>
            <p>
              <strong>Motivation &amp; Challenge:</strong> When acquiring emotion-related physiological signals, subjects must remain seated for extended periods while exposed to high-intensity stimuli to generate emotional responses. However, uncontrollable factors such as perspiration, changes in posture, or the drying of the conductive gel may cause sensors to slip or experience poor contact, resulting in incomplete multimodal signal acquisition to varying degrees. Such incompleteness amplifies the distribution shifts in emotion-related physiological data and ultimately leads to degraded model performance. Emotion-related physiological data also inherently exhibit distribution shifts between the training and test sets. These shifts arise from within-subject variations in emotional state, cognitive load, and environmental conditions across sessions. The unavoidable presence of missing data disrupts the original data patterns, skews feature distributions, and introduces bias, which collectively amplify the existing distribution shifts in physiological data. This amplification makes it increasingly difficult for pre-trained models to capture emotional patterns accurately. Existing methods mainly focus on training-phase strategies to deal with the missing data problem. For example, previous work proposes feature-level and decision-level fusion methods to address missingness in multimodal emotion recognition, but these approaches usually require retraining to adapt to new missing data patterns, which limits their flexibility. A more promising direction is to mitigate the effects of missing data by fine-tuning the model during the testing phase. Recent test-time adaptation methods adjust self-attention modules to assign lower weights to the corrupted modality when dealing with unimodal corruption. However, these methods cannot handle multimodal corruption where multiple physiological signals are simultaneously missing. Therefore, mitigating the negative impact of missing multimodal data on emotion recognition models remains a significant practical challenge.
            </p>
            <p>
              <strong>Our Approach:</strong> 这里概括 MMTTA 的关键思想，包括基于两级熵的样本筛选策略、如何优先选择分布偏移较弱且信息较完整的样本进行更新，以及利用跨模态互信息共享在模态间传播可靠信息，从而实现更加平滑稳健的测试时适应，在缺失模态场景下仍保持良好的情感识别性能。
            </p>
          </div>
        </div>
      </section>

      <!-- Experimental Results 区 -->
      <section class="section">
        <h2 class="section-title">Experimental Results</h2>

        <!-- 小节 1：experiment-1.png -->
        <div class="method-block">
          <div class="figure-box small">
            <img src="experiment-1.png" class="exp1-img" alt="Experimental results overview" />
          </div>
          <div class="fig-caption">
            Figure 3: Overall emotion recognition performance of BiM-TTA and comparison methods under different missing-modality settings.
          </div>
          <div class="section-content">
            <h3 class="section-subtitle">1. Overall Performance under Missing Modalities</h3>
            <p>
              这里简要说明 <code>experiment-1.png</code> 展示的结果，例如在不同缺失模态比例或缺失模式下，BiM-TTA 相比传统 TTA 方法和无适应基线在准确率、F1 等指标上普遍取得更高性能，并且在高缺失率场景中仍保持稳定优势。
            </p>
          </div>
        </div>

        <!-- 小节 2：experiment-2.1.png 和 experiment-2.2.png -->
        <div class="method-block" style="margin-top:32px">
          <div class="figure-box small">
            <img src="experiment-2.1.png" alt="Ablation study part 1" />
          </div>
          <div class="figure-box small" style="margin-top:16px">
            <img src="experiment-2.2.png" alt="Ablation study part 2" />
          </div>
          <div class="fig-caption">
            Figure 4: Ablation and robustness analysis of BiM-TTA. The two subfigures report the impact of different TTA settings and missing-modality configurations on the final performance.
          </div>
          <div class="section-content">
            <h3 class="section-subtitle">2. Ablation Studies and Robustness Analysis</h3>
            <p>
              这里概括 <code>experiment-2.1.png</code> 和 <code>experiment-2.2.png</code> 的关键信息，例如移除 TTA、去掉两级熵样本筛选或关闭互信息共享后性能明显下降，以及在不同缺失模态组合、缺失比例或不同数据集上的鲁棒性表现，从而支持 BiM-TTA 各个组件设计的必要性。
            </p>
          </div>
        </div>
      </section>

      <!-- Video Presentation 区 -->
      <section class="section section--soft">
        <h2 class="section-title">Video Presentation</h2>
        <div class="video-center">
          <div class="video-tabs">
            <div class="video-tab is-active">English</div>
            <div class="video-tab">中文讲解</div>
          </div>

          <div class="video-box">
            <video controls>
              <source src="video_en.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>

          <div class="btn-row">
            <a class="btn btn-paper" href="#">Download English Slides (.pptx)</a>
            <a class="btn btn-code" href="#">下载中文PPT (.pptx)</a>
          </div>
        </div>
      </section>

      <!-- Poster 区：嵌入同目录下的 poster.pdf -->
      <section class="section">
        <h2 class="section-title">Poster</h2>
        <div class="poster-frame">
          <iframe
            class="pdf-frame pdf-frame-tall"
            src="poster.pdf#view=FitH">
          </iframe>
        </div>
      </section>

      <!-- BibTeX 区 -->
      <section class="section section--soft">
        <h2 class="section-title">BibTeX</h2>
        <pre class="bibtex-block">
@inproceedings{
jia2025a,
title={A Multimodal BiMamba Network with Test-Time Adaptation for Emotion Recognition Based on Physiological Signals},
author={Ziyu Jia and Tingyu Du and Zhengyu Tian and Hongkai Li and Yong Zhang and Chenyu Liu},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=3vLp3J7540}
}
        </pre>
      </section>
    </main>

    <footer>
      Built with simple HTML and CSS. Replace placeholder texts, images, video and poster links with your own content.
    </footer>
  </div>
</body>
</html>
