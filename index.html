<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>A Multimodal BiMamba Network with Test-Time Adaptation for Emotion Recognition Based on Physiological Signals</title>

  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;600;800&display=swap" rel="stylesheet">

  <style>
    :root{
      --max-w: 1200px;
      --muted: #6b7280;
      --accent: #1f2937;
      --bg: #fff;
    }
    html,body{
      height:100%;
      margin:0;
      background:var(--bg);
      font-family:"Montserrat",system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial;
      color:#111827;
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
    }
    .container{
      max-width:var(--max-w);
      margin:0 auto;
      padding:48px 20px;
      display:flex;
      flex-direction:column;
      align-items:center;
    }
    header{
      text-align:center;
      width:100%;
    }
    h1{
      margin:0;
      font-weight:800;
      line-height:1.04;
      letter-spacing:-0.02em;
      font-size:clamp(28px,6.5vw,56px);
      margin-bottom:18px;
    }

    /* 作者行和角标 */
    .authors{
      color:var(--muted);
      font-size:15px;
      line-height:1.5;
      margin-bottom:8px;
    }
    .author-line{
      margin-bottom:4px;
    }
    .authors sup{
      font-size:0.7em;
      vertical-align:super;
      margin-left:2px;
      font-weight:700;
      color:inherit;
    }
    .authors sup span{
      font-weight:700;
    }

    /* 颜色编号定义（现在实际用到 1-5） */
    .tag1{ color:#ef4444; }   /* 1 - red */
    .tag2{ color:#3b82f6; }   /* 2 - blue */
    .tag3{ color:#10b981; }   /* 3 - green */
    .tag4{ color:#f97316; }   /* 4 - orange */
    .tag5{ color:#8b5cf6; }   /* 5 - purple */
    .tag6{ color:#ec4899; }   /* 6 - pink */
    .tag7{ color:#0ea5e9; }   /* 7 - cyan */

    .venue-tag{
      margin:4px 0 6px;
      font-size:14px;
      font-weight:700;
      letter-spacing:0.22em;
      text-transform:uppercase;
      color:#111827;
      font-family:"Courier New",ui-monospace,Menlo,Monaco,Consolas,"Liberation Mono";
    }

    .affil{
      color:var(--muted);
      font-size:14px;
      margin-top:4px;
    }
    .affil .aff-num{
      font-weight:700;
      margin-right:4px;
    }

    .meta{
      margin-top:4px;
      color:var(--muted);
      font-size:14px;
    }

    .btns{
      margin-top:18px;
      display:flex;
      gap:12px;
      justify-content:center;
      align-items:center;
      flex-wrap:wrap;
    }
    .btn{
      display:inline-flex;
      align-items:center;
      gap:8px;
      padding:10px 16px;
      border-radius:999px;
      text-decoration:none;
      font-weight:600;
      font-size:15px;
      box-shadow:0 6px 18px rgba(15,23,42,0.06);
    }
    .btn-paper{
      background:var(--accent);
      color:#fff;
    }
    .btn-code{
      background:transparent;
      color:var(--accent);
      border:1px solid rgba(17,24,39,0.12);
    }
    .btn-icon{
      display:inline-flex;
      align-items:center;
      justify-content:center;
    }
    .btn-icon svg{
      width:16px;
      height:16px;
    }

    main{
      width:100%;
      margin-top:36px;
      display:flex;
      flex-direction:column;
      align-items:center;
      gap:56px;
    }

    .figure-box{
      width:100%;
      border-radius:18px;
      border:3px dashed rgba(107,114,128,0.25);
      padding:22px;
      box-sizing:border-box;
      display:flex;
      justify-content:center;
      align-items:center;
      background:linear-gradient(180deg,rgba(255,255,255,0.6),rgba(255,255,255,0.4));
    }
    .figure-box img{
      max-width:100%;
      height:auto;
      border-radius:8px;
      display:block;
      box-shadow:inset 0 -6px 18px rgba(0,0,0,0.03);
    }

    /* 单独给 paper figure 一个类 */
    .paper-figure{
      max-height:80vh;
      width:auto;
    }

    /* PDF iframe 样式（Poster 区使用） */
    .pdf-frame{
      width:100%;
      border:0;
      border-radius:8px;
      box-shadow:inset 0 -6px 18px rgba(0,0,0,0.03);
    }
    .pdf-frame-wide{
      height:80vh;
    }
    .pdf-frame-tall{
      height:680px;
    }

    /* 只缩小 experiment-1.png */
    .exp1-img{
      width:66%;
      height:auto;
    }

    /* 顶部关键段落高亮卡片 */
    .figure-caption-top{
      max-width:900px;
      margin:16px auto 0;
      padding:14px 18px;
      font-size:17px;
      line-height:1.8;
      color:#111827;
      background:#fefce8;
      border-radius:14px;
      border:1px solid #facc15;
      box-shadow:0 10px 30px rgba(15,23,42,0.08);
    }
    .figure-caption-top strong{
      font-weight:700;
    }

    .section{
      width:100%;
    }
    .section-title{
      text-align:center;
      font-size:26px;
      font-weight:700;
      margin:0 0 18px;
    }

    /* 小标题突出 */
    .section-subtitle{
      font-size:19px;
      font-weight:700;
      margin:0 0 8px;
      color:var(--accent);
      display:inline-block;
      padding:4px 12px;
      border-radius:999px;
      background:rgba(15,23,42,0.06);
    }

    /* Motivation / Our Approach 标签样式 */
    .inline-tag{
      display:inline-block;
      padding:2px 10px;
      margin-right:6px;
      border-radius:999px;
      background:#e5e7eb;
      color:#111827;
      font-weight:700;
      font-size:14px;
      letter-spacing:0.02em;
    }

    .section-content{
      max-width:900px;
      margin:0 auto;
      font-size:16px;
      line-height:1.8;
      color:var(--muted);
    }
    .section--soft{
      padding:32px 20px;
      border-radius:18px;
      background:#f9fafb;
    }

    .method-block{
      max-width:1000px;
      margin:0 auto;
    }

    .figure-box.small{
      padding:16px;
    }

    .fig-caption{
      text-align:center;
      font-size:15px;
      color:var(--muted);
      margin-top:8px;
    }

    .video-center{
      max-width:900px;
      margin:0 auto;
      text-align:center;
    }
    .video-tabs{
      display:inline-flex;
      border-radius:999px;
      border:1px solid #e5e7eb;
      overflow:hidden;
      margin-bottom:18px;
    }
    .video-tab{
      padding:8px 18px;
      font-size:15px;
      cursor:default;
      color:var(--muted);
      white-space:nowrap;
    }
    .video-tab.is-active{
      background:#111827;
      color:#fff;
    }
    .video-box{
      width:100%;
      margin:0 auto 12px;
    }
    .video-box video{
      width:100%;
      border-radius:14px;
      box-shadow:0 12px 32px rgba(15,23,42,0.25);
      background:#000;
    }
    .btn-row{
      margin-top:10px;
      display:flex;
      flex-wrap:wrap;
      justify-content:center;
      gap:12px;
    }

    .poster-frame{
      width:100%;
      max-width:1000px;
      margin:0 auto;
      border-radius:14px;
      overflow:hidden;
      border:1px solid #e5e7eb;
      background:#f9fafb;
    }

    .bibtex-block{
      max-width:1000px;
      margin:0 auto;
      background:#111827;
      color:#e5e7eb;
      padding:18px 20px;
      border-radius:14px;
      font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New";
      font-size:14px;
      overflow-x:auto;
      box-sizing:border-box;
    }

    strong{
      font-weight:600;
    }

    footer{
      margin-top:32px;
      color:var(--muted);
      font-size:14px;
      text-align:center;
    }

    @media (max-width:640px){
      .container{
        padding:28px 14px;
      }
      h1{
        font-size:clamp(22px,8.5vw,34px);
      }
      .authors{
        font-size:14px;
      }
      .btn{
        padding:8px 12px;
        font-size:14px;
      }
      main{
        gap:40px;
      }
      .pdf-frame-wide{
        height:90vh;
      }
      .pdf-frame-tall{
        height:520px;
      }
      .paper-figure{
        max-height:70vh;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>
        A Multimodal BiMamba Network with Test-Time Adaptation<br>
        for Emotion Recognition Based on Physiological Signals
      </h1>

      <div class="authors">
        <div class="author-line">
          Ziyu Jia
          <sup><span class="tag1">1</span></sup>,
          Tingyu Du
          <sup><span class="tag2">2</span></sup>,
          Zhengyu Tian
          <sup><span class="tag3">3</span></sup>,
          Hongkai Li
          <sup><span class="tag3">3</span></sup>,
          Yong Zhang
          <sup><span class="tag4">4</span></sup>,
          Chenyu Liu
          <sup><span class="tag5">5</span></sup>
        </div>

        <div class="venue-tag">NEURIPS 2025</div>

        <div class="affil">
          <span class="aff-num tag1">1</span>
          Institute of Automation, Chinese Academy of Sciences
        </div>
        <div class="affil">
          <span class="aff-num tag2">2</span>
          Institute of Computing Technology, Chinese Academy of Sciences
        </div>
        <div class="affil">
          <span class="aff-num tag3">3</span>
          Beijing Jiaotong University
        </div>
        <div class="affil">
          <span class="aff-num tag4">4</span>
          Huzhou University
        </div>
        <div class="affil">
          <span class="aff-num tag5">5</span>
          Nanyang Technological University
        </div>
      </div>

      <div class="btns">
        <a class="btn btn-paper" href="https://neurips.cc/virtual/2025/loc/san-diego/poster/119989">
          <span class="btn-icon" aria-hidden="true">
            <!-- PDF 图标 -->
            <svg viewBox="0 0 24 24" fill="none">
              <rect x="4" y="3" width="14" height="18" rx="2" ry="2" stroke="white" stroke-width="1.6"/>
              <path d="M14 3v4h4" stroke="white" stroke-width="1.6" stroke-linecap="round" stroke-linejoin="round"/>
              <path d="M7 15h10" stroke="white" stroke-width="1.6" stroke-linecap="round"/>
            </svg>
          </span>
          <span>Paper</span>
        </a>
        <a class="btn btn-code" href="https://anonymous.4open.science/r/BiM-TTA-B604">
          <span class="btn-icon" aria-hidden="true">
            <!-- GitHub 风格线框图标 -->
            <svg viewBox="0 0 24 24" fill="none">
              <path d="M12 3c-4.4 0-8 3.6-8 8 0 3.5 2.3 6.4 5.5 7.4-.1-.4-.1-.8-.1-1.3v-1.2c-2 .4-2.5-.9-2.5-.9-.3-.7-.8-1-0.8-1-.6-.4 0-.4 0-.4.6 0.1 1 .7 1 .7.5.9 1.4.6 1.8.4 0-.4.2-.7.3-.9-1.6-.2-3.3-.8-3.3-3.5 0-.8.3-1.4.7-1.9-.1-.2-.3-.9 0-1.8 0 0 .6-.2 2 .7.6-.2 1.3-.3 2-.3.7 0 1.4.1 2 .3 1.4-.9 2-.7 2-.7.3.9.1 1.6 0 1.8.5.5.7 1.1.7 1.9 0 2.7-1.7 3.3-3.3 3.5.2.2.3.6.3 1.1v1.6c0 .5 0 .9-.1 1.3 3.2-1.1 5.5-4 5.5-7.4 0-4.4-3.6-8-8-8z"
                    stroke="#111827" stroke-width="1.1" stroke-linecap="round" stroke-linejoin="round"/>
            </svg>
          </span>
          <span>Code</span>
        </a>
      </div>
    </header>

    <main>
      <!-- 顶部方法大图：使用同目录下的 paper figure.png -->
      <section class="figure-box">
        <img src="paper%20figure.png" alt="Paper overview figure" class="paper-figure" />
      </section>

      <!-- 顶部 figure 描述高亮卡片 -->
      <div class="figure-caption-top">
        <strong>
          In this paper, we propose a multimodal BiMamba network with TTA for emotion recognition.
          The multimodal BiMamba network effectively captures intra-modal dependencies and inter-modal correlations of multimodal physiological signals.
          TTA consists of two-level entropy-based sample filtering and mutual information sharing across modalities, which together achieve smooth adaptation to the target domain and reduce distribution shifts across modalities, thereby alleviating the negative impact of amplified distribution shifts caused by missing multimodal data.
        </strong>
      </div>

      <!-- 1. Abstract 区：灰 -->
      <section class="section section--soft">
        <h2 class="section-title">Abstract</h2>
        <div class="section-content">
          <p>
             Emotion recognition based on physiological signals plays a vital role in psychological health and human–computer interaction, particularly with the substantial advances in multimodal emotion recognition techniques. However, two key challenges remain unresolved: <strong>1) how to effectively model the intra-modal long-range dependencies and inter-modal correlations in multimodal physiological emotion signals</strong>, and <strong>2) how to address the performance limitations resulting from missing multimodal data</strong>. In this paper, we propose <strong>a multimodal bidirectional Mamba (BiMamba) network with test-time adaptation (TTA)</strong> for emotion recognition named <strong>BiM-TTA</strong>. Specifically, <strong>BiM-TTA</strong> consists of <strong>a multimodal BiMamba network</strong> and <strong>a multimodal TTA</strong>. The former includes <strong>intra-modal and inter-modal BiMamba modules</strong>, which model long-range dependencies along the time dimension and capture cross-modal correlations along the channel dimension, respectively. The latter (TTA) mitigates the amplified distribution shifts caused by missing multimodal data through <strong>two-level entropy-based sample filtering</strong> and <strong>mutual information sharing across modalities</strong>. By addressing these challenges, BiM-TTA achieves <strong>state-of-the-art</strong> results on two multimodal emotion datasets.
          </p>
        </div>
      </section>

      <!-- 2. Methodology 区：白 -->
      <section class="section">
        <h2 class="section-title">Methodology</h2>

        <!-- 方法 1：Multimodal BiMamba Network -->
        <div class="method-block">
          <div class="figure-box small">
            <img src="BiMamba-module.png" alt="BiMamba backbone illustration" />
          </div>
          <div class="fig-caption">
            Figure 1: The intra- and inter-modal BiMamba modules capture and fuse the features of different modalities, respectively. "Concat" represents the concatenation of multiple feature vectors u<sub>1</sub>, u<sub>2</sub>, ..., u<sub>M</sub>. T denotes matrix transposition.
          </div>
          <div class="section-content">
            <h3 class="section-subtitle">1. Multimodal BiMamba Network</h3>
            <p>
              <span class="inline-tag">Motivation &amp; Challenge</span>
              <strong> As a typical form of time-series data, emotion-related physiological signals exhibit long-range dependencies reflecting the gradual accumulation of emotional changes.</strong>
              Emotions such as anxiety develop progressively and require a duration for their physiological manifestations to accumulate. This accumulation indicates that emotional changes are not just an instantaneous state but a process that evolves.
              <strong> Inter-modal correlations are also evident in the way different modalities respond.</strong>
              For instance, increases in EEG activity during emotional arousal are often accompanied by galvanic skin response conductance peaks and decreases in electrocardiogram heart-rate variability.
              <strong> Traditional backbone networks have limitations in modeling intra-modal long-range dependencies and inter-modal correlations.</strong>
            </p>
            <p>
              <span class="inline-tag">Our Approach</span>
              Mamba’s selective input mechanism efficiently models long-range dependencies, and its explicit global state variables as part of its state space model (SSM) capture inter-modal correlations simultaneously, thereby effectively addressing the first challenge.
              <strong> We design a multimodal BiMamba network, where the intra-modal BiMamba module models long-range dependencies within modalities, and the inter-modal BiMamba module captures inter-modal correlations.</strong>
            </p>
          </div>
        </div>

        <!-- 方法 2：MMTTA -->
        <div class="method-block" style="margin-top:32px">
          <div class="figure-box small">
            <img src="MMTTA.png" alt="MMTTA module illustration" />
          </div>
          <div class="fig-caption">
            Figure 2: Samples with weak distribution shifts and rich multimodal information are selected for adaptation, while the remaining samples are retained until the next iteration. Then, mutual information sharing across modalities is performed to match the information between different modalities effectively.
          </div>
          <div class="section-content">
            <h3 class="section-subtitle">2. Test-Time Adaptation</h3>
            <p>
              <span class="inline-tag">Motivation &amp; Challenge</span>
              When acquiring emotion-related physiological signals, subjects must remain seated for extended periods while exposed to high-intensity stimuli to generate emotional responses.
              <strong> However, uncontrollable factors such as perspiration, changes in posture, or the drying of the conductive gel may cause sensors to slip or experience poor contact, resulting in incomplete multimodal signal acquisition to varying degrees.</strong>
              Such incompleteness amplifies the distribution shifts in emotion-related physiological data and ultimately leads to degraded model performance. Emotion-related physiological data also inherently exhibit distribution shifts between the training and test sets. These shifts arise from within-subject variations in emotional state, cognitive load, and environmental conditions across sessions. The unavoidable presence of missing data disrupts the original data patterns, skews feature distributions, and introduces bias, which collectively amplify the existing distribution shifts in physiological data.
              <strong> This amplification makes it increasingly difficult for pre-trained models to capture emotional patterns accurately.</strong>
              <strong> Existing methods mainly focus on training-phase strategies to deal with the missing data problem.</strong>
            </p>
            <p>
              <span class="inline-tag">Our Approach</span>
              TTA requires only minimal parameter fine-tuning to mitigate distribution shifts, thus resolving the second challenge.
              <strong> We propose a multimodal TTA method that alleviates the negative impact of amplified distribution shifts caused by missing multimodal data on model performance.</strong>
            </p>
          </div>
        </div>
      </section>

      <!-- 3. Experimental Results 区：灰 -->
      <section class="section section--soft">
        <h2 class="section-title">Experimental Results</h2>

        <!-- 小节 1 -->
        <div class="method-block">
          <div class="section-content">
            <h3 class="section-subtitle">1. Experiments without Missing Data</h3>
          </div>
          <div class="figure-box small">
            <img src="experiment-1.png" class="exp1-img" alt="Experimental results overview" />
          </div>
          <div class="fig-caption">
            Table 1: Comparison of emotion recognition baselines on DEAP and MAHNOB-HCI in terms of valence and arousal accuracy.
          </div>
          <div class="section-content">
            <p>
              For experiments without missing data, as shown in Table 1, BiM-TTA models long-range dependencies within modalities and correlations between modalities through the intra-modal BiMamba module and inter-modal BiMamba module. It achieves the best performance on both datasets, demonstrating the superiority of the multimodal BiMamba backbone network.
            </p>
          </div>
        </div>

        <!-- 小节 2 -->
        <div class="method-block" style="margin-top:32px">
          <div class="section-content">
            <h3 class="section-subtitle">2. Experiments with Missing Data</h3>
          </div>
          <div class="figure-box small">
            <img src="experiment-2.1.png" alt="Ablation study part 1" />
          </div>
          <div class="figure-box small" style="margin-top:16px">
            <img src="experiment-2.2.png" alt="Ablation study part 2" />
          </div>
          <div class="fig-caption">
            Table 2 and 3: Comparative analysis of accuracy for different TTA methods on DEAP and Mahnob-HCI with missing data, relative to the baseline method "No Adapt". The No Adapt baseline corresponds to a model pretrained on the source domain and directly evaluated on the target domain without any adaptation. Results report the improvement rate of each TTA method over "No Adapt" at mask ratios of 0.2, 0.4, 0.6, and 0.8, along with the average improvement.
          </div>
          <div class="section-content">
            <p>
              For experiments with missing data, Table 2 and 3 present the performance improvements of BiM-TTA and other TTA methods relative to the "No Adapt" method. "No Adapt" denotes a model trained on the source domain and subsequently evaluated on the target domain containing missing data, without any adaptation. Our method uses two-level entropy-based sample filtering to avoid the model directly adapting to samples with strong distribution shifts and limited multimodal information. Furthermore, we utilize inter-modal information sharing to align the information between different modalities, facilitating the full adaptation of the model.
            </p>
          </div>
        </div>
      </section>

      <!-- 4. Video Presentation 区：白 -->
      <section class="section">
        <h2 class="section-title">Video Presentation</h2>
        <div class="video-center">
          <div class="video-tabs">
            <div class="video-tab is-active">English</div>
            <div class="video-tab">中文讲解</div>
          </div>

          <div class="video-box">
            <video controls>
              <source src="video_en.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>

          <div class="btn-row">
            <a class="btn btn-paper" href="#">Download English Slides (.pptx)</a>
            <a class="btn btn-code" href="#">下载中文PPT (.pptx)</a>
          </div>
        </div>
      </section>

      <!-- 5. Poster 区：灰 -->
      <section class="section section--soft">
        <h2 class="section-title">Poster</h2>
        <div class="poster-frame">
          <iframe
            class="pdf-frame pdf-frame-tall"
            src="poster.pdf#view=FitH">
          </iframe>
        </div>
      </section>

      <!-- 6. BibTeX 区：白 -->
      <section class="section">
        <h2 class="section-title">BibTeX</h2>
        <pre class="bibtex-block">
@inproceedings{
jia2025a,
title={A Multimodal BiMamba Network with Test-Time Adaptation for Emotion Recognition Based on Physiological Signals},
author={Ziyu Jia and Tingyu Du and Zhengyu Tian and Hongkai Li and Yong Zhang and Chenyu Liu},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=3vLp3J7540}
}
        </pre>
      </section>
    </main>

  </div>
</body>
</html>
