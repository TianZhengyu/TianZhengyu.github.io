<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>A Multimodal BiMamba Network with Test-Time Adaptation for Emotion Recognition Based on Physiological Signals</title>

  <!-- 字重包含 900，给标题等用 -->
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;600;800;900&display=swap" rel="stylesheet">

  <style>
    :root{
      --max-w: 1400px;
      --muted: #000000;
      --accent: #000000;
      --bg: #ffffff;
      --text: #000000;
    }
    html,body{
      height:100%;
      margin:0;
      background:var(--bg);
      font-family:"Montserrat",system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial;
      color:var(--text);
      font-weight:400;
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
    }
    .container{
      max-width:var(--max-w);
      margin:0 auto;
      padding:40px 12px;
      display:flex;
      flex-direction:column;
      align-items:center;
    }
    header{
      text-align:center;
      width:100%;
    }

    h1{
      margin:0;
      font-weight:900;
      line-height:1.14;
      letter-spacing:-0.015em;
      font-size:clamp(20px,3.2vw,36px);
      margin-bottom:14px;
      color:var(--text);
      max-width:1100px;
      margin-left:auto;
      margin-right:auto;
      text-wrap:balance;
    }

    .authors{
      color:var(--text);
      font-size:15px;
      line-height:1.5;
      margin-bottom:8px;
      font-weight:500;
    }
    .author-line{
      margin-bottom:4px;
    }
    .authors sup{
      font-size:0.7em;
      vertical-align:super;
      margin-left:2px;
      font-weight:800;
      color:inherit;
    }
    .authors sup span{
      font-weight:800;
    }

    .tag1{ color:#ef4444; }
    .tag2{ color:#3b82f6; }
    .tag3{ color:#10b981; }
    .tag4{ color:#f97316; }
    .tag5{ color:#8b5cf6; }
    .tag6{ color:#ec4899; }
    .tag7{ color:#0ea5e9; }

    .venue-tag{
      margin:4px 0 6px;
      font-size:14px;
      font-weight:700;
      letter-spacing:0.22em;
      text-transform:uppercase;
      color:var(--text);
      font-family:"Courier New",ui-monospace,Menlo,Monaco,Consolas,"Liberation Mono";
    }

    .affil{
      color:var(--text);
      font-size:14px;
      margin-top:4px;
      font-weight:500;
    }
    .affil .aff-num{
      font-weight:800;
      margin-right:4px;
    }

    .meta{
      margin-top:4px;
      color:var(--muted);
      font-size:14px;
      font-weight:600;
    }

    .btns{
      margin-top:16px;
      display:flex;
      gap:12px;
      justify-content:center;
      align-items:center;
      flex-wrap:wrap;
    }
    .btn{
      display:inline-flex;
      align-items:center;
      gap:8px;
      padding:10px 16px;
      border-radius:999px;
      text-decoration:none;
      font-weight:800;
      font-size:15px;
      box-shadow:0 6px 18px rgba(15,23,42,0.06);
    }
    .btn-paper{
      background:var(--accent);
      color:#ffffff;
    }
    .btn-code{
      background:transparent;
      color:var(--accent);
      border:1px solid rgba(17,24,39,0.18);
    }
    .btn-icon{
      display:inline-flex;
      align-items:center;
      justify-content:center;
    }
    .btn-icon svg{
      width:16px;
      height:16px;
    }

    main{
      width:100%;
      margin-top:32px;
      display:flex;
      flex-direction:column;
      align-items:center;
      gap:56px;
    }

    .figure-box{
      width:100%;
      border-radius:18px;
      border:3px dashed rgba(107,114,128,0.25);
      padding:22px;
      box-sizing:border-box;
      display:flex;
      justify-content:center;
      align-items:center;
      background:linear-gradient(180deg,rgba(255,255,255,0.6),rgba(255,255,255,0.4));
    }
    .figure-box img{
      max-width:100%;
      height:auto;
      border-radius:8px;
      display:block;
      box-shadow:inset 0 -6px 18px rgba(0,0,0,0.03);
    }

    .paper-figure{
      max-height:80vh;
      width:auto;
    }

    .exp1-img{
      width:66%;
      height:auto;
    }

    .contrib-box{
      max-width:980px;
      margin:16px auto 0;
      padding:18px 22px;
      background:linear-gradient(180deg,#fefce8,#fff7ed);
      border-radius:16px;
      border:1.5px solid #f59e0b;
      box-shadow:0 12px 34px rgba(15,23,42,0.10);
      text-align:left;
    }
    .contrib-title{
      font-size:18px;
      font-weight:900;
      color:var(--text);
      letter-spacing:0.08em;
      text-transform:uppercase;
      margin-bottom:10px;
      display:flex;
      align-items:center;
      gap:8px;
    }
    .contrib-list{
      margin:0;
      padding-left:20px;
      font-size:17px;
      line-height:1.75;
      color:var(--text);
      font-weight:500;
    }
    .contrib-list li{
      margin:8px 0;
    }
    .contrib-list strong{
      font-weight:800;
    }

    .section{
      width:100%;
    }
    .section-title{
      text-align:center;
      font-size:42px;
      font-weight:900;
      margin:0 0 18px;
      color:var(--text);
    }

    .section-subtitle{
      font-size:24px;
      font-weight:900;
      margin:0 0 8px;
      color:var(--accent);
      display:inline-block;
      padding:4px 12px;
      border-radius:999px;
      background:rgba(0,0,0,0.06);
    }

    .inline-tag{
      display:inline-block;
      padding:2px 10px;
      margin-right:6px;
      border-radius:999px;
      background:#e5e7eb;
      color:#000000;
      font-weight:800;
      font-size:14px;
      letter-spacing:0.02em;
    }

    .section-content{
      max-width:900px;
      margin:0 auto;
      font-size:16px;
      line-height:1.8;
      color:var(--text);
      font-weight:500;
    }

    /* 让 Methodology 里 Figure 和小标题之间有额外间距 */
    .method-block .section-content{
      margin:20px auto 0;
    }

    .section--soft{
      padding:24px 12px;
      border-radius:18px;
      background:#f9fafb;
    }

    .method-block{
      max-width:1000px;
      margin:0 auto;
    }

    .figure-box.small{
      padding:16px;
    }

    .fig-caption{
      text-align:center;
      font-size:15px;
      color:var(--text);
      margin-top:8px;
      font-weight:500;
    }

    .video-center{
      max-width:900px;
      margin:0 auto;
      text-align:center;
    }
    .video-tabs{
      display:inline-flex;
      border-radius:999px;
      border:1px solid #e5e7eb;
      overflow:hidden;
      margin-bottom:18px;
    }
    .video-tab{
      padding:8px 18px;
      font-size:15px;
      cursor:pointer;
      color:var(--muted);
      white-space:nowrap;
      font-weight:600;
      user-select:none;
    }
    .video-tab.is-active{
      background:#000000;
      color:#ffffff;
      font-weight:900;
    }
    .video-box{
      width:100%;
      margin:0 auto 12px;
    }
    .video-box iframe{
      width:100%;
      aspect-ratio:16/9;
      border-radius:14px;
      box-shadow:0 12px 32px rgba(15,23,42,0.25);
      background:#000000;
      border:0;
    }

    .btn-row{
      margin-top:10px;
      display:flex;
      flex-wrap:wrap;
      justify-content:center;
      gap:12px;
    }

    .poster-frame{
      width:100%;
      max-width:1000px;
      margin:0 auto;
      border-radius:14px;
      overflow:hidden;
      border:1px solid #e5e7eb;
      background:#f9fafb;
    }
    .poster-img{
      width:100%;
      height:auto;
      display:block;
    }

    .bibtex-block{
      max-width:1000px;
      margin:0 auto;
      background:#ffffff;
      color:#000000;
      padding:18px 20px;
      border-radius:14px;
      border:1px solid #e5e7eb;
      font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New";
      font-size:14px;
      overflow-x:auto;
      box-sizing:border-box;
      font-weight:500;
    }

    strong{
      font-weight:550;
      color:var(--text);
    }

    @media (max-width:640px){
      .container{
        padding:28px 10px;
      }
      h1{
        font-size:clamp(18px,4.8vw,28px);
        line-height:1.2;
      }
      .authors{
        font-size:14px;
      }
      .btn{
        padding:8px 12px;
        font-size:14px;
      }
      main{
        gap:40px;
      }
      .paper-figure{
        max-height:70vh;
      }
      .contrib-box{
        padding:14px 16px;
      }
      .contrib-list{
        font-size:16px;
      }
      .section--soft{
        padding:20px 10px;
      }
      .section-title{
        font-size:32px;
      }
      .section-subtitle{
        font-size:20px;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>
        A Multimodal BiMamba Network with Test-Time Adaptation<br>
        for Emotion Recognition Based on Physiological Signals
      </h1>

      <div class="authors">
        <div class="author-line">
          Ziyu Jia
          <sup><span class="tag1">1</span></sup>,
          Tingyu Du
          <sup><span class="tag2">2</span></sup>,
          Zhengyu Tian
          <sup><span class="tag3">3</span></sup>,
          Hongkai Li
          <sup><span class="tag3">3</span></sup>,
          Yong Zhang
          <sup><span class="tag4">4</span></sup>,
          Chenyu Liu
          <sup><span class="tag5">5</span></sup>
        </div>

        <div class="venue-tag">NEURIPS 2025</div>

        <div class="affil">
          <span class="aff-num tag1">1</span>
          Institute of Automation, Chinese Academy of Sciences
        </div>
        <div class="affil">
          <span class="aff-num tag2">2</span>
          Institute of Computing Technology, Chinese Academy of Sciences
        </div>
        <div class="affil">
          <span class="aff-num tag3">3</span>
          Beijing Jiaotong University
        </div>
        <div class="affil">
          <span class="aff-num tag4">4</span>
          Huzhou University
        </div>
        <div class="affil">
          <span class="aff-num tag5">5</span>
          Nanyang Technological University
        </div>
      </div>

      <div class="btns">
        <a class="btn btn-paper" href="https://neurips.cc/virtual/2025/loc/san-diego/poster/119989">
          <span class="btn-icon" aria-hidden="true">
            <svg viewBox="0 0 24 24" fill="none">
              <rect x="4" y="3" width="14" height="18" rx="2" ry="2" stroke="white" stroke-width="1.6"/>
              <path d="M14 3v4h4" stroke="white" stroke-width="1.6" stroke-linecap="round" stroke-linejoin="round"/>
              <path d="M7 15h10" stroke="white" stroke-width="1.6" stroke-linecap="round"/>
            </svg>
          </span>
          <span>Paper</span>
        </a>
        <a class="btn btn-code" href="https://anonymous.4open.science/r/BiM-TTA-B604">
          <span class="btn-icon" aria-hidden="true">
            <svg viewBox="0 0 24 24" fill="none">
              <path d="M12 3c-4.4 0-8 3.6-8 8 0 3.5 2.3 6.4 5.5 7.4-.1-.4-.1-.8-.1-1.3v-1.2c-2 .4-2.5-.9-2.5-.9-.3-.7-.8-1-0.8-1-.6-.4 0-.4 0-.4.6 0.1 1 .7 1 .7.5.9 1.4.6 1.8.4 0-.4.2-.7.3-.9-1.6-.2-3.3-.8-3.3-3.5 0-.8.3-1.4.7-1.9-.1-.2-.3-.9 0-1.8 0 0 .6-.2 2 .7.6-.2 1.3-.3 2-.3.7 0 1.4.1 2 .3 1.4-.9 2-.7 2-.7.3.9.1 1.6 0 1.8.5.5.7 1.1.7 1.9 0 2.7-1.7 3.3-3.3 3.5.2.2.3.6.3 1.1v1.6c0 .5 0 .9-.1 1.3 3.2-1.1 5.5-4 5.5-7.4 0-4.4-3.6-8-8-8z"
                    stroke="#ffffff" stroke-width="1.1" stroke-linecap="round" stroke-linejoin="round"/>
            </svg>
          </span>
          <span>Code</span>
        </a>
      </div>
    </header>

    <main>
      <section class="figure-box">
        <img src="paper%20figure.png" alt="Paper overview figure" class="paper-figure" />
      </section>

      <div class="contrib-box">
        <div class="contrib-title">
          Contributions
        </div>
        <ol class="contrib-list">
          <li>
            <strong>Multimodal BiMamba backbone.</strong>
            A multimodal BiMamba network is designed, where the intra-modal BiMamba module models long-range dependencies within modalities and the inter-modal BiMamba module captures inter-modal correlations.
          </li>
          <li>
            <strong>Multimodal test-time adaptation.</strong>
            A multimodal TTA method is proposed to alleviate the negative impact of amplified distribution shifts caused by missing multimodal data on model performance.
          </li>
          <li>
            <strong>State-of-the-art validation.</strong>
            Evaluation on two multimodal emotion datasets confirms state-of-the-art performance and verifies the effectiveness of BiM-TTA.
          </li>
        </ol>
      </div>

      <section class="section section--soft">
        <h2 class="section-title">Abstract</h2>
        <div class="section-content">
          <p>
            Emotion recognition based on physiological signals plays a vital role in psychological health and human–computer interaction, particularly with the substantial advances in multimodal emotion recognition techniques. However, two key challenges remain unresolved: <br>
            <strong>1) how to effectively model the intra-modal long-range dependencies and inter-modal correlations in multimodal physiological emotion signals.</strong><br>
            <strong>2) how to address the performance limitations resulting from missing multimodal data.</strong><br>
            <strong>In this paper, we propose a multimodal bidirectional Mamba (BiMamba) network with test-time adaptation (TTA) for emotion recognition named BiM-TTA</strong>. <br>
            Specifically, BiM-TTA consists of a multimodal BiMamba network and a multimodal TTA. The former includes intra-modal and inter-modal BiMamba modules, which model long-range dependencies along the time dimension and capture cross-modal correlations along the channel dimension, respectively. The latter (TTA) mitigates the amplified distribution shifts caused by missing multimodal data through two-level entropy-based sample filtering and mutual information sharing across modalities. By addressing these challenges, <strong>BiM-TTA achieves state-of-the-art results on two multimodal emotion datasets</strong>.
          </p>
        </div>
      </section>

      <section class="section">
        <h2 class="section-title">Methodology</h2>

        <div class="method-block">
          <div class="figure-box small">
            <img src="BiMamba-module.png" alt="BiMamba backbone illustration" />
          </div>
          <div class="fig-caption">
            Figure 1: The intra- and inter-modal BiMamba modules capture and fuse the features of different modalities, respectively. "Concat" represents the concatenation of multiple feature vectors u<sub>1</sub>, u<sub>2</sub>, ..., u<sub>M</sub>. T denotes matrix transposition.
          </div>
          <div class="section-content">
            <h3 class="section-subtitle">1. Multimodal BiMamba Network</h3>
            <p>
              <span class="inline-tag">Challenge</span>
              <strong> As a typical form of time-series data, emotion-related physiological signals exhibit long-range dependencies reflecting the gradual accumulation of emotional changes.
              Inter-modal correlations are also evident in the way different modalities respond.
              Traditional backbone networks have limitations in modeling intra-modal long-range dependencies and inter-modal correlations.</strong>
            </p>
            <p>
              <span class="inline-tag">Our Approach</span>
              <strong> Mamba’s selective input mechanism efficiently models long-range dependencies, and its explicit global state variables as part of its state space model (SSM) capture inter-modal correlations simultaneously, thereby effectively addressing the first challenge.
              We design a multimodal BiMamba network, where the intra-modal BiMamba module models long-range dependencies within modalities, and the inter-modal BiMamba module captures inter-modal correlations.</strong>
            </p>
          </div>
        </div>

        <div class="method-block" style="margin-top:32px">
          <div class="figure-box small">
            <img src="MMTTA.png" alt="MMTTA module illustration" />
          </div>
          <div class="fig-caption">
            Figure 2: Samples with weak distribution shifts and rich multimodal information are selected for adaptation, while the remaining samples are retained until the next iteration. Then, mutual information sharing across modalities is performed to match the information between different modalities effectively.
          </div>
          <div class="section-content">
            <h3 class="section-subtitle">2. Test-Time Adaptation</h3>
            <p>
              <span class="inline-tag">Challenge</span>
              <strong> When acquiring emotion-related physiological signals, subjects must remain seated for extended periods while exposed to high-intensity stimuli to generate emotional responses.
              However, uncontrollable factors such as perspiration, changes in posture, or the drying of the conductive gel may cause sensors to slip or experience poor contact, resulting in incomplete multimodal signal acquisition to varying degrees.</strong>
              <strong> The unavoidable presence of missing data disrupts the original data patterns, skews feature distributions, and introduces bias, which collectively amplify the existing distribution shifts in physiological data. This amplification makes it increasingly difficult for pre-trained models to capture emotional patterns accurately.</strong>
              <strong> Existing methods mainly focus on training-phase strategies to deal with the missing data problem.</strong>
            </p>
            <p>
              <span class="inline-tag">Our Approach</span>
              <strong> TTA requires only minimal parameter fine-tuning to mitigate distribution shifts, thus resolving the second challenge.
              We propose a multimodal TTA method that alleviates the negative impact of amplified distribution shifts caused by missing multimodal data on model performance.</strong>
            </p>
          </div>
        </div>
      </section>

      <section class="section section--soft">
        <h2 class="section-title">Experimental Results</h2>

        <div class="method-block">
          <div class="section-content">
            <h3 class="section-subtitle">1. Experiments without Missing Data</h3>
          </div>
          <div class="figure-box small">
            <img src="experiment-1.png" class="exp1-img" alt="Experimental results overview" />
          </div>
          <div class="fig-caption">
            Table 1: Comparison of emotion recognition baselines on DEAP and MAHNOB-HCI in terms of valence and arousal accuracy.
          </div>
          <div class="section-content">
            <p>
              For experiments without missing data, as shown in Table 1, BiM-TTA models long-range dependencies within modalities and correlations between modalities through the intra-modal BiMamba module and inter-modal BiMamba module. It achieves the best performance on both datasets, demonstrating the superiority of the multimodal BiMamba backbone network.
            </p>
          </div>
        </div>

        <div class="method-block" style="margin-top:32px">
          <div class="section-content">
            <h3 class="section-subtitle">2. Experiments with Missing Data</h3>
          </div>
          <div class="figure-box small">
            <img src="experiment-2.1.png" alt="Ablation study part 1" />
          </div>
          <div class="figure-box small" style="margin-top:16px">
            <img src="experiment-2.2.png" alt="Ablation study part 2" />
          </div>
          <div class="fig-caption">
            Table 2 and 3: Comparative analysis of accuracy for different TTA methods on DEAP and Mahnob-HCI with missing data, relative to the baseline method "No Adapt". The No Adapt baseline corresponds to a model pretrained on the source domain and directly evaluated on the target domain without any adaptation. Results report the improvement rate of each TTA method over "No Adapt" at mask ratios of 0.2, 0.4, 0.6, and 0.8, along with the average improvement.
          </div>
          <div class="section-content">
            <p>
              For experiments with missing data, Table 2 and 3 present the performance improvements of BiM-TTA and other TTA methods relative to the "No Adapt" method. "No Adapt" denotes a model trained on the source domain and subsequently evaluated on the target domain containing missing data, without any adaptation. Our method uses two-level entropy-based sample filtering to prevent the model from directly adapting to samples that exhibit strong distribution shifts and contain limited multimodal information. Furthermore, we utilize inter-modal information sharing to align the information between different modalities, facilitating the full adaptation of the model.
            </p>
          </div>
        </div>
      </section>

      <section class="section">
        <h2 class="section-title">Video Presentation</h2>
        <div class="video-center">
          <div class="video-tabs">
            <div class="video-tab is-active" data-target="video-en">English</div>
            <div class="video-tab" data-target="video-zh">中文讲解</div>
          </div>

          <!-- English video -->
          <div class="video-box" id="video-en">
            <iframe
              src="https://player.bilibili.com/player.html?bvid=BV1xXm8BLEuU"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen>
            </iframe>
          </div>

          <!-- Chinese video -->
          <div class="video-box" id="video-zh" style="display:none;">
            <iframe
              src="https://player.bilibili.com/player.html?bvid=BV11Xm8BLEkA"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen>
            </iframe>
          </div>

          <div class="btn-row">
            <!-- English PPTX 下载 -->
            <a class="btn btn-paper" href="BiM-TTA-en.pptx" download="BiM-TTA-en.pptx">
              Download English Slides (.pptx)
            </a>
            <!-- 中文 PPTX 下载 -->
            <a class="btn btn-code" href="BiM-TTA-ch.pptx" download="BiM-TTA-ch.pptx">
              下载中文PPT (.pptx)
            </a>
          </div>
        </div>
      </section>

      <section class="section section--soft">
        <h2 class="section-title">Poster</h2>
        <div class="poster-frame">
          <img src="poster.png" alt="Poster" class="poster-img" />
        </div>
      </section>

      <section class="section">
        <h2 class="section-title">BibTeX</h2>
        <pre class="bibtex-block">
@inproceedings{
jia2025a,
title={A Multimodal BiMamba Network with Test-Time Adaptation for Emotion Recognition Based on Physiological Signals},
author={Ziyu Jia and Tingyu Du and Zhengyu Tian and Hongkai Li and Yong Zhang and Chenyu Liu},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=3vLp3J7540}
}
        </pre>
      </section>
    </main>

  </div>

  <script>
    // 简单 Tab 切换逻辑
    const videoTabs = document.querySelectorAll('.video-tab');
    const videoEn = document.getElementById('video-en');
    const videoZh = document.getElementById('video-zh');

    videoTabs.forEach(tab => {
      tab.addEventListener('click', () => {
        videoTabs.forEach(t => t.classList.remove('is-active'));
        tab.classList.add('is-active');

        if (tab.dataset.target === 'video-en') {
          videoEn.style.display = 'block';
          videoZh.style.display = 'none';
        } else {
          videoEn.style.display = 'none';
          videoZh.style.display = 'block';
        }
      });
    });
  </script>
</body>
</html>
